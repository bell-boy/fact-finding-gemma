{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import path_patch, plain_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma = LanguageModel(\"google/gemma-2-2b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "gemma.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.model',\n",
       " '.model.embed_tokens',\n",
       " '.model.layers',\n",
       " '.model.layers.0',\n",
       " '.model.layers.0.self_attn',\n",
       " '.model.layers.0.self_attn.q_proj',\n",
       " '.model.layers.0.self_attn.k_proj',\n",
       " '.model.layers.0.self_attn.v_proj',\n",
       " '.model.layers.0.self_attn.o_proj',\n",
       " '.model.layers.0.self_attn.rotary_emb',\n",
       " '.model.layers.0.mlp',\n",
       " '.model.layers.0.mlp.gate_proj',\n",
       " '.model.layers.0.mlp.up_proj',\n",
       " '.model.layers.0.mlp.down_proj',\n",
       " '.model.layers.0.mlp.act_fn',\n",
       " '.model.layers.0.input_layernorm',\n",
       " '.model.layers.0.post_attention_layernorm',\n",
       " '.model.layers.0.pre_feedforward_layernorm',\n",
       " '.model.layers.0.post_feedforward_layernorm',\n",
       " '.model.layers.1',\n",
       " '.model.layers.1.self_attn',\n",
       " '.model.layers.1.self_attn.q_proj',\n",
       " '.model.layers.1.self_attn.k_proj',\n",
       " '.model.layers.1.self_attn.v_proj',\n",
       " '.model.layers.1.self_attn.o_proj',\n",
       " '.model.layers.1.self_attn.rotary_emb',\n",
       " '.model.layers.1.mlp',\n",
       " '.model.layers.1.mlp.gate_proj',\n",
       " '.model.layers.1.mlp.up_proj',\n",
       " '.model.layers.1.mlp.down_proj',\n",
       " '.model.layers.1.mlp.act_fn',\n",
       " '.model.layers.1.input_layernorm',\n",
       " '.model.layers.1.post_attention_layernorm',\n",
       " '.model.layers.1.pre_feedforward_layernorm',\n",
       " '.model.layers.1.post_feedforward_layernorm',\n",
       " '.model.layers.2',\n",
       " '.model.layers.2.self_attn',\n",
       " '.model.layers.2.self_attn.q_proj',\n",
       " '.model.layers.2.self_attn.k_proj',\n",
       " '.model.layers.2.self_attn.v_proj',\n",
       " '.model.layers.2.self_attn.o_proj',\n",
       " '.model.layers.2.self_attn.rotary_emb',\n",
       " '.model.layers.2.mlp',\n",
       " '.model.layers.2.mlp.gate_proj',\n",
       " '.model.layers.2.mlp.up_proj',\n",
       " '.model.layers.2.mlp.down_proj',\n",
       " '.model.layers.2.mlp.act_fn',\n",
       " '.model.layers.2.input_layernorm',\n",
       " '.model.layers.2.post_attention_layernorm',\n",
       " '.model.layers.2.pre_feedforward_layernorm',\n",
       " '.model.layers.2.post_feedforward_layernorm',\n",
       " '.model.layers.3',\n",
       " '.model.layers.3.self_attn',\n",
       " '.model.layers.3.self_attn.q_proj',\n",
       " '.model.layers.3.self_attn.k_proj',\n",
       " '.model.layers.3.self_attn.v_proj',\n",
       " '.model.layers.3.self_attn.o_proj',\n",
       " '.model.layers.3.self_attn.rotary_emb',\n",
       " '.model.layers.3.mlp',\n",
       " '.model.layers.3.mlp.gate_proj',\n",
       " '.model.layers.3.mlp.up_proj',\n",
       " '.model.layers.3.mlp.down_proj',\n",
       " '.model.layers.3.mlp.act_fn',\n",
       " '.model.layers.3.input_layernorm',\n",
       " '.model.layers.3.post_attention_layernorm',\n",
       " '.model.layers.3.pre_feedforward_layernorm',\n",
       " '.model.layers.3.post_feedforward_layernorm',\n",
       " '.model.layers.4',\n",
       " '.model.layers.4.self_attn',\n",
       " '.model.layers.4.self_attn.q_proj',\n",
       " '.model.layers.4.self_attn.k_proj',\n",
       " '.model.layers.4.self_attn.v_proj',\n",
       " '.model.layers.4.self_attn.o_proj',\n",
       " '.model.layers.4.self_attn.rotary_emb',\n",
       " '.model.layers.4.mlp',\n",
       " '.model.layers.4.mlp.gate_proj',\n",
       " '.model.layers.4.mlp.up_proj',\n",
       " '.model.layers.4.mlp.down_proj',\n",
       " '.model.layers.4.mlp.act_fn',\n",
       " '.model.layers.4.input_layernorm',\n",
       " '.model.layers.4.post_attention_layernorm',\n",
       " '.model.layers.4.pre_feedforward_layernorm',\n",
       " '.model.layers.4.post_feedforward_layernorm',\n",
       " '.model.layers.5',\n",
       " '.model.layers.5.self_attn',\n",
       " '.model.layers.5.self_attn.q_proj',\n",
       " '.model.layers.5.self_attn.k_proj',\n",
       " '.model.layers.5.self_attn.v_proj',\n",
       " '.model.layers.5.self_attn.o_proj',\n",
       " '.model.layers.5.self_attn.rotary_emb',\n",
       " '.model.layers.5.mlp',\n",
       " '.model.layers.5.mlp.gate_proj',\n",
       " '.model.layers.5.mlp.up_proj',\n",
       " '.model.layers.5.mlp.down_proj',\n",
       " '.model.layers.5.mlp.act_fn',\n",
       " '.model.layers.5.input_layernorm',\n",
       " '.model.layers.5.post_attention_layernorm',\n",
       " '.model.layers.5.pre_feedforward_layernorm',\n",
       " '.model.layers.5.post_feedforward_layernorm',\n",
       " '.model.layers.6',\n",
       " '.model.layers.6.self_attn',\n",
       " '.model.layers.6.self_attn.q_proj',\n",
       " '.model.layers.6.self_attn.k_proj',\n",
       " '.model.layers.6.self_attn.v_proj',\n",
       " '.model.layers.6.self_attn.o_proj',\n",
       " '.model.layers.6.self_attn.rotary_emb',\n",
       " '.model.layers.6.mlp',\n",
       " '.model.layers.6.mlp.gate_proj',\n",
       " '.model.layers.6.mlp.up_proj',\n",
       " '.model.layers.6.mlp.down_proj',\n",
       " '.model.layers.6.mlp.act_fn',\n",
       " '.model.layers.6.input_layernorm',\n",
       " '.model.layers.6.post_attention_layernorm',\n",
       " '.model.layers.6.pre_feedforward_layernorm',\n",
       " '.model.layers.6.post_feedforward_layernorm',\n",
       " '.model.layers.7',\n",
       " '.model.layers.7.self_attn',\n",
       " '.model.layers.7.self_attn.q_proj',\n",
       " '.model.layers.7.self_attn.k_proj',\n",
       " '.model.layers.7.self_attn.v_proj',\n",
       " '.model.layers.7.self_attn.o_proj',\n",
       " '.model.layers.7.self_attn.rotary_emb',\n",
       " '.model.layers.7.mlp',\n",
       " '.model.layers.7.mlp.gate_proj',\n",
       " '.model.layers.7.mlp.up_proj',\n",
       " '.model.layers.7.mlp.down_proj',\n",
       " '.model.layers.7.mlp.act_fn',\n",
       " '.model.layers.7.input_layernorm',\n",
       " '.model.layers.7.post_attention_layernorm',\n",
       " '.model.layers.7.pre_feedforward_layernorm',\n",
       " '.model.layers.7.post_feedforward_layernorm',\n",
       " '.model.layers.8',\n",
       " '.model.layers.8.self_attn',\n",
       " '.model.layers.8.self_attn.q_proj',\n",
       " '.model.layers.8.self_attn.k_proj',\n",
       " '.model.layers.8.self_attn.v_proj',\n",
       " '.model.layers.8.self_attn.o_proj',\n",
       " '.model.layers.8.self_attn.rotary_emb',\n",
       " '.model.layers.8.mlp',\n",
       " '.model.layers.8.mlp.gate_proj',\n",
       " '.model.layers.8.mlp.up_proj',\n",
       " '.model.layers.8.mlp.down_proj',\n",
       " '.model.layers.8.mlp.act_fn',\n",
       " '.model.layers.8.input_layernorm',\n",
       " '.model.layers.8.post_attention_layernorm',\n",
       " '.model.layers.8.pre_feedforward_layernorm',\n",
       " '.model.layers.8.post_feedforward_layernorm',\n",
       " '.model.layers.9',\n",
       " '.model.layers.9.self_attn',\n",
       " '.model.layers.9.self_attn.q_proj',\n",
       " '.model.layers.9.self_attn.k_proj',\n",
       " '.model.layers.9.self_attn.v_proj',\n",
       " '.model.layers.9.self_attn.o_proj',\n",
       " '.model.layers.9.self_attn.rotary_emb',\n",
       " '.model.layers.9.mlp',\n",
       " '.model.layers.9.mlp.gate_proj',\n",
       " '.model.layers.9.mlp.up_proj',\n",
       " '.model.layers.9.mlp.down_proj',\n",
       " '.model.layers.9.mlp.act_fn',\n",
       " '.model.layers.9.input_layernorm',\n",
       " '.model.layers.9.post_attention_layernorm',\n",
       " '.model.layers.9.pre_feedforward_layernorm',\n",
       " '.model.layers.9.post_feedforward_layernorm',\n",
       " '.model.layers.10',\n",
       " '.model.layers.10.self_attn',\n",
       " '.model.layers.10.self_attn.q_proj',\n",
       " '.model.layers.10.self_attn.k_proj',\n",
       " '.model.layers.10.self_attn.v_proj',\n",
       " '.model.layers.10.self_attn.o_proj',\n",
       " '.model.layers.10.self_attn.rotary_emb',\n",
       " '.model.layers.10.mlp',\n",
       " '.model.layers.10.mlp.gate_proj',\n",
       " '.model.layers.10.mlp.up_proj',\n",
       " '.model.layers.10.mlp.down_proj',\n",
       " '.model.layers.10.mlp.act_fn',\n",
       " '.model.layers.10.input_layernorm',\n",
       " '.model.layers.10.post_attention_layernorm',\n",
       " '.model.layers.10.pre_feedforward_layernorm',\n",
       " '.model.layers.10.post_feedforward_layernorm',\n",
       " '.model.layers.11',\n",
       " '.model.layers.11.self_attn',\n",
       " '.model.layers.11.self_attn.q_proj',\n",
       " '.model.layers.11.self_attn.k_proj',\n",
       " '.model.layers.11.self_attn.v_proj',\n",
       " '.model.layers.11.self_attn.o_proj',\n",
       " '.model.layers.11.self_attn.rotary_emb',\n",
       " '.model.layers.11.mlp',\n",
       " '.model.layers.11.mlp.gate_proj',\n",
       " '.model.layers.11.mlp.up_proj',\n",
       " '.model.layers.11.mlp.down_proj',\n",
       " '.model.layers.11.mlp.act_fn',\n",
       " '.model.layers.11.input_layernorm',\n",
       " '.model.layers.11.post_attention_layernorm',\n",
       " '.model.layers.11.pre_feedforward_layernorm',\n",
       " '.model.layers.11.post_feedforward_layernorm',\n",
       " '.model.layers.12',\n",
       " '.model.layers.12.self_attn',\n",
       " '.model.layers.12.self_attn.q_proj',\n",
       " '.model.layers.12.self_attn.k_proj',\n",
       " '.model.layers.12.self_attn.v_proj',\n",
       " '.model.layers.12.self_attn.o_proj',\n",
       " '.model.layers.12.self_attn.rotary_emb',\n",
       " '.model.layers.12.mlp',\n",
       " '.model.layers.12.mlp.gate_proj',\n",
       " '.model.layers.12.mlp.up_proj',\n",
       " '.model.layers.12.mlp.down_proj',\n",
       " '.model.layers.12.mlp.act_fn',\n",
       " '.model.layers.12.input_layernorm',\n",
       " '.model.layers.12.post_attention_layernorm',\n",
       " '.model.layers.12.pre_feedforward_layernorm',\n",
       " '.model.layers.12.post_feedforward_layernorm',\n",
       " '.model.layers.13',\n",
       " '.model.layers.13.self_attn',\n",
       " '.model.layers.13.self_attn.q_proj',\n",
       " '.model.layers.13.self_attn.k_proj',\n",
       " '.model.layers.13.self_attn.v_proj',\n",
       " '.model.layers.13.self_attn.o_proj',\n",
       " '.model.layers.13.self_attn.rotary_emb',\n",
       " '.model.layers.13.mlp',\n",
       " '.model.layers.13.mlp.gate_proj',\n",
       " '.model.layers.13.mlp.up_proj',\n",
       " '.model.layers.13.mlp.down_proj',\n",
       " '.model.layers.13.mlp.act_fn',\n",
       " '.model.layers.13.input_layernorm',\n",
       " '.model.layers.13.post_attention_layernorm',\n",
       " '.model.layers.13.pre_feedforward_layernorm',\n",
       " '.model.layers.13.post_feedforward_layernorm',\n",
       " '.model.layers.14',\n",
       " '.model.layers.14.self_attn',\n",
       " '.model.layers.14.self_attn.q_proj',\n",
       " '.model.layers.14.self_attn.k_proj',\n",
       " '.model.layers.14.self_attn.v_proj',\n",
       " '.model.layers.14.self_attn.o_proj',\n",
       " '.model.layers.14.self_attn.rotary_emb',\n",
       " '.model.layers.14.mlp',\n",
       " '.model.layers.14.mlp.gate_proj',\n",
       " '.model.layers.14.mlp.up_proj',\n",
       " '.model.layers.14.mlp.down_proj',\n",
       " '.model.layers.14.mlp.act_fn',\n",
       " '.model.layers.14.input_layernorm',\n",
       " '.model.layers.14.post_attention_layernorm',\n",
       " '.model.layers.14.pre_feedforward_layernorm',\n",
       " '.model.layers.14.post_feedforward_layernorm',\n",
       " '.model.layers.15',\n",
       " '.model.layers.15.self_attn',\n",
       " '.model.layers.15.self_attn.q_proj',\n",
       " '.model.layers.15.self_attn.k_proj',\n",
       " '.model.layers.15.self_attn.v_proj',\n",
       " '.model.layers.15.self_attn.o_proj',\n",
       " '.model.layers.15.self_attn.rotary_emb',\n",
       " '.model.layers.15.mlp',\n",
       " '.model.layers.15.mlp.gate_proj',\n",
       " '.model.layers.15.mlp.up_proj',\n",
       " '.model.layers.15.mlp.down_proj',\n",
       " '.model.layers.15.mlp.act_fn',\n",
       " '.model.layers.15.input_layernorm',\n",
       " '.model.layers.15.post_attention_layernorm',\n",
       " '.model.layers.15.pre_feedforward_layernorm',\n",
       " '.model.layers.15.post_feedforward_layernorm',\n",
       " '.model.layers.16',\n",
       " '.model.layers.16.self_attn',\n",
       " '.model.layers.16.self_attn.q_proj',\n",
       " '.model.layers.16.self_attn.k_proj',\n",
       " '.model.layers.16.self_attn.v_proj',\n",
       " '.model.layers.16.self_attn.o_proj',\n",
       " '.model.layers.16.self_attn.rotary_emb',\n",
       " '.model.layers.16.mlp',\n",
       " '.model.layers.16.mlp.gate_proj',\n",
       " '.model.layers.16.mlp.up_proj',\n",
       " '.model.layers.16.mlp.down_proj',\n",
       " '.model.layers.16.mlp.act_fn',\n",
       " '.model.layers.16.input_layernorm',\n",
       " '.model.layers.16.post_attention_layernorm',\n",
       " '.model.layers.16.pre_feedforward_layernorm',\n",
       " '.model.layers.16.post_feedforward_layernorm',\n",
       " '.model.layers.17',\n",
       " '.model.layers.17.self_attn',\n",
       " '.model.layers.17.self_attn.q_proj',\n",
       " '.model.layers.17.self_attn.k_proj',\n",
       " '.model.layers.17.self_attn.v_proj',\n",
       " '.model.layers.17.self_attn.o_proj',\n",
       " '.model.layers.17.self_attn.rotary_emb',\n",
       " '.model.layers.17.mlp',\n",
       " '.model.layers.17.mlp.gate_proj',\n",
       " '.model.layers.17.mlp.up_proj',\n",
       " '.model.layers.17.mlp.down_proj',\n",
       " '.model.layers.17.mlp.act_fn',\n",
       " '.model.layers.17.input_layernorm',\n",
       " '.model.layers.17.post_attention_layernorm',\n",
       " '.model.layers.17.pre_feedforward_layernorm',\n",
       " '.model.layers.17.post_feedforward_layernorm',\n",
       " '.model.layers.18',\n",
       " '.model.layers.18.self_attn',\n",
       " '.model.layers.18.self_attn.q_proj',\n",
       " '.model.layers.18.self_attn.k_proj',\n",
       " '.model.layers.18.self_attn.v_proj',\n",
       " '.model.layers.18.self_attn.o_proj',\n",
       " '.model.layers.18.self_attn.rotary_emb',\n",
       " '.model.layers.18.mlp',\n",
       " '.model.layers.18.mlp.gate_proj',\n",
       " '.model.layers.18.mlp.up_proj',\n",
       " '.model.layers.18.mlp.down_proj',\n",
       " '.model.layers.18.mlp.act_fn',\n",
       " '.model.layers.18.input_layernorm',\n",
       " '.model.layers.18.post_attention_layernorm',\n",
       " '.model.layers.18.pre_feedforward_layernorm',\n",
       " '.model.layers.18.post_feedforward_layernorm',\n",
       " '.model.layers.19',\n",
       " '.model.layers.19.self_attn',\n",
       " '.model.layers.19.self_attn.q_proj',\n",
       " '.model.layers.19.self_attn.k_proj',\n",
       " '.model.layers.19.self_attn.v_proj',\n",
       " '.model.layers.19.self_attn.o_proj',\n",
       " '.model.layers.19.self_attn.rotary_emb',\n",
       " '.model.layers.19.mlp',\n",
       " '.model.layers.19.mlp.gate_proj',\n",
       " '.model.layers.19.mlp.up_proj',\n",
       " '.model.layers.19.mlp.down_proj',\n",
       " '.model.layers.19.mlp.act_fn',\n",
       " '.model.layers.19.input_layernorm',\n",
       " '.model.layers.19.post_attention_layernorm',\n",
       " '.model.layers.19.pre_feedforward_layernorm',\n",
       " '.model.layers.19.post_feedforward_layernorm',\n",
       " '.model.layers.20',\n",
       " '.model.layers.20.self_attn',\n",
       " '.model.layers.20.self_attn.q_proj',\n",
       " '.model.layers.20.self_attn.k_proj',\n",
       " '.model.layers.20.self_attn.v_proj',\n",
       " '.model.layers.20.self_attn.o_proj',\n",
       " '.model.layers.20.self_attn.rotary_emb',\n",
       " '.model.layers.20.mlp',\n",
       " '.model.layers.20.mlp.gate_proj',\n",
       " '.model.layers.20.mlp.up_proj',\n",
       " '.model.layers.20.mlp.down_proj',\n",
       " '.model.layers.20.mlp.act_fn',\n",
       " '.model.layers.20.input_layernorm',\n",
       " '.model.layers.20.post_attention_layernorm',\n",
       " '.model.layers.20.pre_feedforward_layernorm',\n",
       " '.model.layers.20.post_feedforward_layernorm',\n",
       " '.model.layers.21',\n",
       " '.model.layers.21.self_attn',\n",
       " '.model.layers.21.self_attn.q_proj',\n",
       " '.model.layers.21.self_attn.k_proj',\n",
       " '.model.layers.21.self_attn.v_proj',\n",
       " '.model.layers.21.self_attn.o_proj',\n",
       " '.model.layers.21.self_attn.rotary_emb',\n",
       " '.model.layers.21.mlp',\n",
       " '.model.layers.21.mlp.gate_proj',\n",
       " '.model.layers.21.mlp.up_proj',\n",
       " '.model.layers.21.mlp.down_proj',\n",
       " '.model.layers.21.mlp.act_fn',\n",
       " '.model.layers.21.input_layernorm',\n",
       " '.model.layers.21.post_attention_layernorm',\n",
       " '.model.layers.21.pre_feedforward_layernorm',\n",
       " '.model.layers.21.post_feedforward_layernorm',\n",
       " '.model.layers.22',\n",
       " '.model.layers.22.self_attn',\n",
       " '.model.layers.22.self_attn.q_proj',\n",
       " '.model.layers.22.self_attn.k_proj',\n",
       " '.model.layers.22.self_attn.v_proj',\n",
       " '.model.layers.22.self_attn.o_proj',\n",
       " '.model.layers.22.self_attn.rotary_emb',\n",
       " '.model.layers.22.mlp',\n",
       " '.model.layers.22.mlp.gate_proj',\n",
       " '.model.layers.22.mlp.up_proj',\n",
       " '.model.layers.22.mlp.down_proj',\n",
       " '.model.layers.22.mlp.act_fn',\n",
       " '.model.layers.22.input_layernorm',\n",
       " '.model.layers.22.post_attention_layernorm',\n",
       " '.model.layers.22.pre_feedforward_layernorm',\n",
       " '.model.layers.22.post_feedforward_layernorm',\n",
       " '.model.layers.23',\n",
       " '.model.layers.23.self_attn',\n",
       " '.model.layers.23.self_attn.q_proj',\n",
       " '.model.layers.23.self_attn.k_proj',\n",
       " '.model.layers.23.self_attn.v_proj',\n",
       " '.model.layers.23.self_attn.o_proj',\n",
       " '.model.layers.23.self_attn.rotary_emb',\n",
       " '.model.layers.23.mlp',\n",
       " '.model.layers.23.mlp.gate_proj',\n",
       " '.model.layers.23.mlp.up_proj',\n",
       " '.model.layers.23.mlp.down_proj',\n",
       " '.model.layers.23.mlp.act_fn',\n",
       " '.model.layers.23.input_layernorm',\n",
       " '.model.layers.23.post_attention_layernorm',\n",
       " '.model.layers.23.pre_feedforward_layernorm',\n",
       " '.model.layers.23.post_feedforward_layernorm',\n",
       " '.model.layers.24',\n",
       " '.model.layers.24.self_attn',\n",
       " '.model.layers.24.self_attn.q_proj',\n",
       " '.model.layers.24.self_attn.k_proj',\n",
       " '.model.layers.24.self_attn.v_proj',\n",
       " '.model.layers.24.self_attn.o_proj',\n",
       " '.model.layers.24.self_attn.rotary_emb',\n",
       " '.model.layers.24.mlp',\n",
       " '.model.layers.24.mlp.gate_proj',\n",
       " '.model.layers.24.mlp.up_proj',\n",
       " '.model.layers.24.mlp.down_proj',\n",
       " '.model.layers.24.mlp.act_fn',\n",
       " '.model.layers.24.input_layernorm',\n",
       " '.model.layers.24.post_attention_layernorm',\n",
       " '.model.layers.24.pre_feedforward_layernorm',\n",
       " '.model.layers.24.post_feedforward_layernorm',\n",
       " '.model.layers.25',\n",
       " '.model.layers.25.self_attn',\n",
       " '.model.layers.25.self_attn.q_proj',\n",
       " '.model.layers.25.self_attn.k_proj',\n",
       " '.model.layers.25.self_attn.v_proj',\n",
       " '.model.layers.25.self_attn.o_proj',\n",
       " '.model.layers.25.self_attn.rotary_emb',\n",
       " '.model.layers.25.mlp',\n",
       " '.model.layers.25.mlp.gate_proj',\n",
       " '.model.layers.25.mlp.up_proj',\n",
       " '.model.layers.25.mlp.down_proj',\n",
       " '.model.layers.25.mlp.act_fn',\n",
       " '.model.layers.25.input_layernorm',\n",
       " '.model.layers.25.post_attention_layernorm',\n",
       " '.model.layers.25.pre_feedforward_layernorm',\n",
       " '.model.layers.25.post_feedforward_layernorm',\n",
       " '.model.norm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in gemma.model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_logits = path_patch(\n",
    "  gemma, \n",
    "  [(\".model.layers.25.post_attention_layernorm\", -1),\n",
    "   (\".model.layers.25.post_feedforward_layernorm\", -1)], \n",
    "  [\".model.norm\"],\n",
    "  \"goodbye\",\n",
    "  \"goodbye\"\n",
    ")\n",
    "\n",
    "clean_logits = plain_run(gemma, \"goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(patched_logits - clean_logits).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
