{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from nnsight import LanguageModel\n",
    "from src.utils import path_patch, plain_run\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma = LanguageModel(\"google/gemma-2-2b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "gemma.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_prompt = \"Fact: Tiger Woods plays the sport of golf.\\nFact: {} plays the sport of\"\n",
    "athlete_data = pd.read_csv(\"../data/athlete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gemma.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(26):\n",
    "  for comp in [\"attention\", \"feedforward\"]:\n",
    "    # I want to select a random pair of distinct sports n times\n",
    "    sports = [\"American football\", \"basketball\", \"baseball\"]\n",
    "    for _ in range(6):\n",
    "      src_sport, target_sport = sample(sports, 2)\n",
    "\n",
    "      src_athlete = athlete_data[athlete_data[\"Sport\"] == src_sport].sample(n=1).iloc[0]\n",
    "      target_athlete = athlete_data[athlete_data[\"Sport\"] == target_sport].sample(n=1).iloc[0]\n",
    "\n",
    "      src_prompt = sport_prompt.format(src_athlete[\"Name\"])\n",
    "      target_prompt = sport_prompt.format(target_athlete[\"Name\"])\n",
    "\n",
    "      logit_diff = lambda logits: logits[:, -1, tokenizer.convert_tokens_to_ids(src_athlete[\"Sport Token\"])] - logits[:, -1, tokenizer.convert_tokens_to_ids(target_athlete[\"Sport Token\"])]\n",
    "\n",
    "      src_diff = logit_diff(plain_run(gemma, src_prompt))\n",
    "      patched_diff = logit_diff(path_patch(\n",
    "        gemma,\n",
    "        [(f\".model.layers.{layer}.post_{comp}_layernorm\",-1)],\n",
    "        [(f\".model.norm\",-1)],\n",
    "        src_prompt,\n",
    "        target_prompt\n",
    "      ))\n",
    "\n",
    "      \n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e2289fe594badb620d2f180c4c1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    }
   ],
   "source": [
    "patched_logits = path_patch(\n",
    "  gemma, \n",
    "  [(\".model.layers.25.post_attention_layernorm\", -1),\n",
    "   (\".model.layers.25.post_feedforward_layernorm\", -1)], \n",
    "  [\".model.norm\"],\n",
    "  \"goodbye\",\n",
    "  \"goodbye\"\n",
    ")\n",
    "\n",
    "clean_logits = plain_run(gemma, \"goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gemma.trace(\"testing!\") as tracer, torch.no_grad():\n",
    "  gemma.model.layers[25].post_attention_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2RMSNorm((2304,), eps=1e-06)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma.model.layers[25].post_attention_layernorm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
